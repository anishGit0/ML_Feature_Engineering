{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. A parameter is a variable that allows customization and flexibility in functions, models, or reports. It acts as an input that controls behavior without changing the underlying structure.\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "cQMz4QL-Fw8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Correlation is a statistical measure that describes the relationship between two variables. It shows how one variable changes in relation to another.\n",
        "\n",
        "  A negative correlation means that as one variable increases, the other decreases, and vice versa. It indicates an inverse relationship between two variables.\n",
        "\n",
        "  Example:\n",
        "  *   Temperature & Sweater Sales → As temperature rises, sweater sales drop.\n",
        "  *   Exercise & Body Fat → More exercise leads to less body fat.\n",
        "\n",
        "  Negative correlation values range from -1 to 0\n"
      ],
      "metadata": {
        "id": "AkzkfNWBGASL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Machine Learning (ML) is a branch of artificial intelligence (AI) that enables computers to learn patterns from data and make decisions without explicit programming.\n",
        "\n",
        "  Main Components of Machine Learning:\n",
        "  *   Data → Raw information used for training the model.\n",
        "  *   Features → Relevant variables (independent factors) extracted from data.\n",
        "  *   Model → An algorithm that learns from data patterns.\n",
        "  *   Training → Process of feeding data to the model to learn.\n",
        "  *   Evaluation → Assessing model performance using metrics.\n",
        "  *   Prediction → Using the trained model to make future predictions.\n",
        "  *   Optimization → Tuning hyperparameters to improve accuracy."
      ],
      "metadata": {
        "id": "jOlj5fi9GlQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. The loss value measures how far a model's predictions are from the actual values. A lower loss means a better model, while a higher loss indicates poor performance.\n",
        "\n",
        "  How It Helps:\n",
        "  *   Training Progress → Loss decreases as the model learns.List item\n",
        "  *   Overfitting Detection → If training loss is low but validation loss is high, the model is overfitting.\n",
        "  *   Model Selection → Comparing loss values helps choose the best model.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OHvTzgeCHUQw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Continuous Variables:\n",
        "\n",
        "  *   Can take infinite values within a range.\n",
        "  *   Measured on a scale (e.g., height, weight, temperature).\n",
        "  *   Example: \"Age = 25.4 years\"\n",
        "\n",
        "  Categorical Variables:\n",
        "  *   Represent distinct groups or categories.\n",
        "  *   Can be nominal (no order, e.g., colors) or ordinal (ordered, e.g., education levels).\n",
        "  *   Example: \"City = New York\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vj3XabDYHxDb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Since machine learning models work with numerical data, categorical variables need to be converted into a numerical format.\n",
        "\n",
        "  Here are common techniques:\n",
        "  *   Encoding Techniques\n",
        "  *   Feature Engineering Techniques\n",
        "\n"
      ],
      "metadata": {
        "id": "yvmZwBbLIVqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Training Dataset → Used to train the model by learning patterns from data.\n",
        "\n",
        "  Testing Dataset → Used to evaluate model performance on unseen data."
      ],
      "metadata": {
        "id": "-kf5T_njI7kn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. **sklearn.preprocessing** is a module in Scikit-Learn that provides tools for transforming and normalizing data before training a machine learning model."
      ],
      "metadata": {
        "id": "RFxt8hqcJFtE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. A test set is a portion of the dataset used to evaluate a trained machine learning model. It contains unseen data that helps measure how well the model generalizes to new inputs.\n",
        "\n",
        "  Example:\n",
        "\n",
        "  If you have 10,000 data points:\n",
        "  *   Train Set (80%) → 8,000 samples (used for learning).\n",
        "  *   Test Set (20%) → 2,000 samples (used for evaluation).\n",
        "\n"
      ],
      "metadata": {
        "id": "xrIa1TnRKIhU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Splitting Data for Model Fitting in Python:\n",
        "\n",
        "  Use train_test_split from sklearn.model_selection:\n",
        "  ```\n",
        "  # This is formatted as code\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "  ```\n",
        "\n",
        "  Approach to a Machine Learning Problem:\n",
        "  *   Define the Problem → Understand the goal and business need.\n",
        "  *   Collect Data → Gather relevant datasets.\n",
        "  *   Data Preprocessing → Clean, handle missing values, and encode categorical data.\n",
        "  *  Exploratory Data Analysis (EDA) → Analyze patterns and correlations.\n",
        "  *   Feature Engineering → Select or create useful features.\n",
        "  *   Split Data → Train-Test split to avoid overfitting.\n",
        "  *   Choose & Train Model → Select an algorithm and fit it on the training data.\n",
        "  *   Evaluate Model → Use metrics (accuracy, RMSE, precision, recall).\n",
        "  *   Hyperparameter Tuning → Optimize performance using GridSearchCV, RandomizedSearchCV.\n",
        "  *   Deploy Model → Deploy and monitor performance in real-world scenarios."
      ],
      "metadata": {
        "id": "Ri6iFg0UKe0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. EDA helps understand, clean, and prepare data before training a machine learning model. Key reasons include:\n",
        "\n",
        "  *   Identify Data Issues → Detect missing values, outliers, and errors.\n",
        "  *   Understand Data Distribution → Analyze feature distributions and relationships.\n",
        "  *   Feature Selection & Engineering → Choose relevant features and create new ones.\n",
        "  *   Detect Correlation & Multicollinearity → Avoid redundant features.\n",
        "  *   Improve Model Performance → Clean and well-prepared data leads to better accuracy."
      ],
      "metadata": {
        "id": "mB9xmz_jLzXC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Correlation measures the relationship between two variables, showing how one variable changes with respect to another.\n",
        "\n",
        "  Types of Correlation:\n",
        "\n",
        "  *   Positive Correlation (+1)\n",
        "  *   Negative Correlation (-1)\n",
        "  *   No Correlation (0)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dskI1XwAMcA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. A negative correlation means that as one variable increases, the other decreases, and vice versa. It indicates an inverse relationship between two variables.\n",
        "\n",
        "  Example:\n",
        "\n",
        "  Temperature & Sweater Sales → As temperature rises, sweater sales drop.\n",
        "  \n",
        "  Exercise & Body Fat → More exercise leads to less body fat.\n",
        "  Negative correlation values range from -1 to 0"
      ],
      "metadata": {
        "id": "zPXHlFOUMy6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Use Pandas to compute correlation:\n",
        "\n",
        "  ```\n",
        "  # This is formatted as code\n",
        "  import pandas as pd\n",
        "\n",
        "  # Sample DataFrame\n",
        "  data = {'A': [1, 2, 3, 4, 5], 'B': [2, 4, 6, 8, 10]}\n",
        "  df = pd.DataFrame(data)\n",
        "\n",
        "  # Compute correlation\n",
        "  correlation_matrix = df.corr()\n",
        "  print(correlation_matrix)\n",
        "\n",
        "  ```\n",
        "\n"
      ],
      "metadata": {
        "id": "pYkjkgN3NCNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Causation means that a change in one variable directly causes a change in another.\n",
        "\n",
        "  Correlation:\n",
        "  *   \tMeasures relationship between two variables\n",
        "  *   Can be positive, negative, or zero\n",
        "  *   Does not imply cause\n",
        "\n",
        "  Causation:\n",
        "  *   One variable directly affects another\n",
        "  *   Always has a cause-and-effect\n",
        "  *   Requires controlled experiments"
      ],
      "metadata": {
        "id": "fokgySBoNXto"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. An optimizer is an algorithm that adjusts a machine learning model’s parameters (weights) to minimize the loss function and improve accuracy.\n",
        "\n",
        "  Types of Optimizers:\n",
        "  *   Gradient Descent (GD)\n",
        "  *   Stochastic Gradient Descent (SGD)\n",
        "  *   Momentum\n",
        "  *   Adam (Adaptive Moment Estimation)\n",
        "  *   RMSprop (Root Mean Square Propagation)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1cQj-sMSOE6D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. sklearn.linear_model is a module in Scikit-Learn that provides various linear models for regression and classification tasks."
      ],
      "metadata": {
        "id": "j4U35xdJOpGd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. model.fit() trains a machine learning model by learning patterns from the input data. It adjusts model parameters (like weights) to minimize the error.\n",
        "\n",
        "  Required Arguments:\n",
        "  \n",
        "  X (Features/Input Data) → Independent variables (e.g., numerical or categorical data).\n",
        "\n",
        "  y (Target/Labels) → Dependent variable (values to predict)."
      ],
      "metadata": {
        "id": "006vtsFkOyYr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. model.predict() makes predictions using the trained model on new/unseen data. It applies the learned patterns (from model.fit()) to generate outputs.\n",
        "\n",
        "  Required Argument:\n",
        "\n",
        "  X (Features/Input Data) → The independent variables for which predictions are needed."
      ],
      "metadata": {
        "id": "zcuILW7gU8pn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Continuous Variables:\n",
        "\n",
        "  *   Can take any numerical value within a range.\n",
        "  *   Example: Height, weight, temperature, price, age\n",
        "  *   Measured on a scale (e.g., 1.5, 2.75, etc.).\n",
        "\n",
        "  Categorical Variables:\n",
        "  *   Represent distinct groups or categories.\n",
        "  *   Example: Gender (Male/Female), City (New York, London, Tokyo)\n",
        "  *   Can be:\n",
        "  \n",
        "      Nominal (No order, e.g., colors: Red, Blue, Green).\n",
        "\n",
        "     Ordinal (Ordered categories, e.g., education level: High School < College < PhD).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T7GpmpHWVOb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Feature scaling is the process of normalizing or standardizing numerical features so they have a similar scale. It ensures that no feature dominates the learning process due to its larger magnitude.\n",
        "\n",
        "  Required for Distance-Based Models → E.g., KNN, SVM, and PCA.\n",
        "\n",
        "  Prevents Bias → Avoids dominance of high-magnitude features.\n",
        "\n",
        "  Improves Model Performance → Helps models converge faster."
      ],
      "metadata": {
        "id": "IYAwIHm8Vyoo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Scaling in Python (Using Scikit-Learn):\n",
        "  *    Standardization (Z-Score Scaling)\n",
        "\n",
        "\n",
        "    ```\n",
        "      # This is formatted as code\n",
        "      from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "      scaler = StandardScaler()\n",
        "      X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    ```\n",
        "\n",
        "\n",
        "  *    Min-Max Scaling (Normalization: 0 to 1)\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sDCv5bY6WGYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. sklearn.preprocessing is a module in Scikit-Learn that provides tools for scaling, encoding, and transforming data before training a machine learning model.\n",
        "\n",
        "  To prepare data correctly for ML models, improving performance and accuracy."
      ],
      "metadata": {
        "id": "zbvNX0ZFWpkX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Use train_test_split from sklearn.model_selection:\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Example dataset\n",
        "X = [[1], [2], [3], [4], [5]]  # Features\n",
        "y = [2, 4, 6, 8, 10]  # Target\n",
        "\n",
        "# Splitting data (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "```\n",
        "  To evaluate model performance on unseen data and avoid overfitting.\n"
      ],
      "metadata": {
        "id": "X9eGveaWW0hg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Data encoding is the process of converting categorical data (like colors or city names) into numerical representations that machine learning models can understand and work with. This is necessary because most machine learning algorithms are designed to process numerical data."
      ],
      "metadata": {
        "id": "mGf-wcz2XGd1"
      }
    }
  ]
}